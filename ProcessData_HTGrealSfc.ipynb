{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "# # Plotting utils \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker \n",
    "import matplotlib.patches as patches\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Analysis\n",
    "import numpy as np \n",
    "import xarray as xr\n",
    "import datetime\n",
    "from   datetime import date, timedelta\n",
    "import pandas as pd \n",
    "import scipy.stats as stats\n",
    "## Need to use metPy conda env\n",
    "import metpy.calc as mpc\n",
    "from metpy.units import units\n",
    "import pickle\n",
    "\n",
    "# Import Ngl with pyn_env active \n",
    "import Ngl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grabbed from Brian M. to use time midpoints, not end periods\n",
    "def cesm_correct_time(ds):\n",
    "    \"\"\"Given a Dataset, check for time_bnds,\n",
    "       and use avg(time_bnds) to replace the time coordinate.\n",
    "       Purpose is to center the timestamp on the averaging inverval.   \n",
    "       NOTE: ds should have been loaded using `decode_times=False`\n",
    "    \"\"\"\n",
    "    assert 'time_bnds' in ds\n",
    "    assert 'time' in ds\n",
    "    correct_time_values = ds['time_bnds'].mean(dim='nbnd')\n",
    "    # copy any metadata:\n",
    "    correct_time_values.attrs = ds['time'].attrs\n",
    "    ds = ds.assign_coords({\"time\": correct_time_values})\n",
    "    ds = xr.decode_cf(ds)  # decode to datetime objects\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define a function to get the height of the PBL as the level with maximum d(var)/dz. \n",
    "Inputs:  A dataset with CAM output ('DS') and the variable to differentiate ('pbl_var')\n",
    "Outputs: An array with boundary layer depth\n",
    "'''\n",
    "def PBLHasMaxDZ(DS, pbl_var): \n",
    "    # Convert HMGcamDS_all to height (nabbed from Rich's script)\n",
    "    p0 = DS['P0'].values[0]\n",
    "    \n",
    "    plevm = DS['hyam']*p0 + DS['hybm']*DS['PS'].isel(lat=0,lon=0) # Mid level\n",
    "    plevm.attrs['units'] = \"Pa\"\n",
    "\n",
    "    # Height with standard atmosphere\n",
    "    zlevm      = plevm\n",
    "    zlevm_vals = 1000.*np.asarray(mpc.pressure_to_height_std(plevm)) # Units of [m] after multiplied \n",
    "    zlevm      = plevm.copy(deep=True)\n",
    "    zlevm[:,:] = zlevm_vals\n",
    "    \n",
    "    pvar        = DS[pbl_var].isel(lat=0,lon=0)\n",
    "    pvar['lev'] = zlevm[0,:].values\n",
    "    dvardz      = pvar.differentiate(\"lev\") # Find field gradient wrt HEIGHT!\n",
    "\n",
    "    dvardz.loc[:,200:]   = 0.  # Restrict to a specificheight region\n",
    "    dvardz.loc[:,:3000.] = 0\n",
    "\n",
    "    nT = np.shape(dvardz)[0]\n",
    "    PBLdepth = np.full([nT], np.nan)\n",
    "\n",
    "    for iT in range(nT):\n",
    "        iLevs  = np.where((zlevm[iT,:]>=200) & (zlevm[iT,:]<=3000))[0]\n",
    "        maxLev = np.where(dvardz[iT,iLevs]==np.nanmax(dvardz[iT,iLevs]))[0]\n",
    "        PBLdepth[iT] = zlevm[iT,iLevs[maxLev[0]]]\n",
    "    \n",
    "    return PBLdepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolateToPressure_v2(DS, varName, pressGoals):\n",
    "#     nCases = len(DSin.case.values)\n",
    "#     nTimes = len(DSin.time.values)\n",
    "    \n",
    "#     saveOut = np.full([nTimes,len(pressGoals),1,1], np.nan)\n",
    "\n",
    "    ## For the larger arrays, need to operate case-by-case; input to vinth2p can only be 3 or 4 dimensions. \n",
    "#     for iCase in range(nCases): \n",
    "#     DS = DSin\n",
    "\n",
    "    p0mb = DS.P0.values[0]/100        # mb\n",
    "\n",
    "    # Pull out hya/hyb profiles \n",
    "    hyam = DS.hyam.values[0,:]\n",
    "    hybm = DS.hybm.values[0,:]\n",
    "    hyai = DS.hyai.values[0,:]\n",
    "    hybi = DS.hybi.values[0,:]\n",
    "\n",
    "    # Surface pressure with time dimension\n",
    "    PS   = DS.PS.values              # Pa \n",
    "\n",
    "    # Converting variables: \n",
    "    if np.shape(DS[varName].values)[1]==len(DS.ilev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyai,hybi,pressGoals,PS,1,p0mb,1,True)\n",
    "    elif np.shape(DS[varName].values)[1]==len(DS.lev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyam,hybm,pressGoals,PS,1,p0mb,1,True)\n",
    "\n",
    "    saveOut = varInterp\n",
    "    \n",
    "    return saveOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with file 1 of 44 \n",
      "Done with file 2 of 44 \n",
      "Done with file 3 of 44 \n",
      "Done with file 4 of 44 \n",
      "Done with file 5 of 44 \n",
      "Done with file 6 of 44 \n",
      "Done with file 7 of 44 \n",
      "Done with file 8 of 44 \n",
      "Done with file 9 of 44 \n",
      "Done with file 10 of 44 \n",
      "Done with file 11 of 44 \n",
      "Done with file 12 of 44 \n",
      "Done with file 13 of 44 \n",
      "Done with file 14 of 44 \n",
      "Done with file 15 of 44 \n",
      "Done with file 16 of 44 \n",
      "Done with file 17 of 44 \n",
      "Done with file 18 of 44 \n",
      "Done with file 19 of 44 \n",
      "Done with file 20 of 44 \n",
      "Done with file 21 of 44 \n",
      "Done with file 22 of 44 \n",
      "Done with file 23 of 44 \n",
      "Done with file 24 of 44 \n",
      "Done with file 25 of 44 \n",
      "Done with file 26 of 44 \n",
      "Done with file 27 of 44 \n",
      "Done with file 28 of 44 \n",
      "Done with file 29 of 44 \n",
      "Done with file 30 of 44 \n",
      "Done with file 31 of 44 \n",
      "Done with file 32 of 44 \n",
      "Done with file 33 of 44 \n",
      "Done with file 34 of 44 \n",
      "Done with file 35 of 44 \n",
      "Done with file 36 of 44 \n",
      "Done with file 37 of 44 \n",
      "Done with file 38 of 44 \n",
      "Done with file 39 of 44 \n",
      "Done with file 40 of 44 \n",
      "Done with file 41 of 44 \n",
      "Done with file 42 of 44 \n",
      "Done with file 43 of 44 \n",
      "Done with file 44 of 44 \n"
     ]
    }
   ],
   "source": [
    "## Read in CLM files \n",
    "\n",
    "# This set uses theta, not theta_v; and adds outputs of rt'thv' and tau\n",
    "# ## HTG * 10 case\n",
    "# dataDir  = '/Users/mdfowler/Documents/Analysis/CLASP/SCAM_runs/FullyCoupledFromCTSM/FinalOption/betterFinidat4_MoreMomentsNoReorder/5minCoupling/realSfc_HTGtimes10/'\n",
    "# caseName = 'FSCAM.T42_T42.CLASP_fullycoupled_FinalOption.onlyThlRt.HTG_betterFinidat4_MoreMomentsNoReorder_5mDt_realSfc10htg_useTheta_'\n",
    "\n",
    "\n",
    "# ## HTG * 0.5 case\n",
    "# dataDir  = '/Users/mdfowler/Documents/Analysis/CLASP/SCAM_runs/FullyCoupledFromCTSM/FinalOption/betterFinidat4_MoreMomentsNoReorder/5minCoupling/realSfc_HTGtimes0p5/'\n",
    "# caseName = 'FSCAM.T42_T42.CLASP_fullycoupled_FinalOption.onlyThlRt.HTG_betterFinidat4_MoreMomentsNoReorder_5mDt_realSfc0p5htg_useTheta_'\n",
    "\n",
    "## HTG * 0.0 case\n",
    "dataDir  = '/Users/mdfowler/Documents/Analysis/CLASP/SCAM_runs/FullyCoupledFromCTSM/FinalOption/betterFinidat4_MoreMomentsNoReorder/5minCoupling/realSfc_HTGtimes0/'\n",
    "caseName = 'FSCAM.T42_T42.CLASP_fullycoupled_FinalOption.onlyThlRt.HTG_betterFinidat4_MoreMomentsNoReorder_5mDt_realSfc0p0htg_useTheta_'\n",
    "\n",
    "\n",
    "yearStrings  = np.asarray(['2015','2016','2017','2018'])\n",
    "dateEndFiles = np.asarray([ '-05-31-84585.nc',\n",
    "                            '-06-09-56985.nc',\n",
    "                            '-06-18-29385.nc',\n",
    "                            '-06-27-01785.nc',\n",
    "                            '-07-05-60585.nc',\n",
    "                            '-07-14-32985.nc',\n",
    "                            '-07-23-05385.nc',\n",
    "                            '-07-31-64185.nc',\n",
    "                            '-08-09-36585.nc',\n",
    "                            '-08-18-08985.nc',\n",
    "                            '-08-26-67785.nc' ])\n",
    "\n",
    "fileCount=0\n",
    "for iYr in range(len(yearStrings)): \n",
    "    fileStart_atm_HTG  = dataDir+caseName+yearStrings[iYr]+'jja_try2.cam.h1.'+yearStrings[iYr]\n",
    "    fileStart_HTG      = dataDir+caseName+yearStrings[iYr]+'jja_try2.clm2.h0.'+yearStrings[iYr]\n",
    "    fileStartPatch_HTG = dataDir+caseName+yearStrings[iYr]+'jja_try2.clm2.h1.'+yearStrings[iYr]\n",
    "        \n",
    "    for iFile in range(len(dateEndFiles)):\n",
    "        fileName_atm_HTG  = fileStart_atm_HTG+dateEndFiles[iFile]\n",
    "        fileName_HTG      = fileStart_HTG+dateEndFiles[iFile]\n",
    "        fileNamePatch_HTG = fileStartPatch_HTG+dateEndFiles[iFile]\n",
    "\n",
    "        with xr.open_dataset(fileName_atm_HTG, decode_times=False) as HTG_camDS:\n",
    "            HTG_camDS = cesm_correct_time(HTG_camDS)\n",
    "            HTG_camDS['time'] = HTG_camDS.indexes['time'].to_datetimeindex()  \n",
    "            \n",
    "        with xr.open_dataset(fileName_HTG, decode_times=True) as HTG_clmDS: \n",
    "            HTG_clmDS['time'] = HTG_camDS['time']\n",
    "\n",
    "        with xr.open_dataset(fileNamePatch_HTG, decode_times=True) as HTG_clmPatchDS: \n",
    "            HTG_clmPatchDS['time'] = HTG_camDS['time']\n",
    "        \n",
    "        # Discard the first two days if iFile == 0  \n",
    "        if iFile==0:\n",
    "            iTimeStart   = np.where(HTG_camDS.time.values >= (HTG_camDS.time.values[0] + np.timedelta64(2,'D')))[0]\n",
    "            timeArr      = np.arange(iTimeStart[0], len(HTG_camDS.time.values))\n",
    "\n",
    "            HTG_camDS      = HTG_camDS.isel(time=timeArr)\n",
    "            HTG_clmDS      = HTG_clmDS.isel(time=timeArr)\n",
    "            HTG_clmPatchDS = HTG_clmPatchDS.isel(time=timeArr)\n",
    "\n",
    "        if fileCount==0:\n",
    "            HTGcamDS_all = HTG_camDS\n",
    "            HTGclmDS_realSfc = HTG_clmDS\n",
    "            HTGclmDS_realSfcPatch = HTG_clmPatchDS\n",
    "            \n",
    "        else: \n",
    "            HTGcamDS_all = xr.concat([HTGcamDS_all,HTG_camDS], dim='time')\n",
    "            HTGclmDS_realSfc = xr.concat([HTGclmDS_realSfc,HTG_clmDS], dim='time', data_vars='minimal')\n",
    "            HTGclmDS_realSfcPatch = xr.concat([HTGclmDS_realSfcPatch,HTG_clmPatchDS], dim='time', data_vars='minimal')\n",
    "        \n",
    "        fileCount = fileCount+1\n",
    "        print('Done with file %i of %i '% (fileCount,len(yearStrings)*len(dateEndFiles)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with file 1 of 4 \n",
      "Done with file 2 of 4 \n",
      "Done with file 3 of 4 \n",
      "Done with file 4 of 4 \n"
     ]
    }
   ],
   "source": [
    "## Add other vars to the list\n",
    "\n",
    "# This set uses theta, not theta_v; and adds outputs of rt'thv' and tau\n",
    "# ## HTG * 10 case\n",
    "# dataDir  = '/Users/mdfowler/Documents/Analysis/CLASP/SCAM_runs/FullyCoupledFromCTSM/FinalOption/betterFinidat4_MoreMomentsNoReorder/5minCoupling/realSfc_HTGtimes10/'\n",
    "# caseName = 'FSCAM.T42_T42.CLASP_fullycoupled_FinalOption.onlyThlRt.HTG_betterFinidat4_MoreMomentsNoReorder_5mDt_realSfc10htg_useTheta_'\n",
    "\n",
    "\n",
    "# ## HTG * 0.5 case\n",
    "# dataDir  = '/Users/mdfowler/Documents/Analysis/CLASP/SCAM_runs/FullyCoupledFromCTSM/FinalOption/betterFinidat4_MoreMomentsNoReorder/5minCoupling/realSfc_HTGtimes0p5/'\n",
    "# caseName = 'FSCAM.T42_T42.CLASP_fullycoupled_FinalOption.onlyThlRt.HTG_betterFinidat4_MoreMomentsNoReorder_5mDt_realSfc0p5htg_useTheta_'\n",
    "\n",
    "## HTG * 0.0 case\n",
    "dataDir  = '/Users/mdfowler/Documents/Analysis/CLASP/SCAM_runs/FullyCoupledFromCTSM/FinalOption/betterFinidat4_MoreMomentsNoReorder/5minCoupling/realSfc_HTGtimes0/'\n",
    "caseName = 'FSCAM.T42_T42.CLASP_fullycoupled_FinalOption.onlyThlRt.HTG_betterFinidat4_MoreMomentsNoReorder_5mDt_realSfc0p0htg_useTheta_'\n",
    "\n",
    "yearStrings  = np.asarray(['2015','2016','2017','2018'])\n",
    "\n",
    "fileCount=0\n",
    "for iYr in range(len(yearStrings)): \n",
    "    \n",
    "    fileStart_atm_HTG = dataDir+caseName+yearStrings[iYr]+'jja_try2.cam.h0.'\n",
    "    fileName_atm_HTG  = fileStart_atm_HTG+'ExtraVarsAndBudget.nc'\n",
    "\n",
    "    with xr.open_dataset(fileName_atm_HTG, decode_times=False) as HTG_camDS:\n",
    "        HTG_camDS = cesm_correct_time(HTG_camDS)\n",
    "        HTG_camDS['time'] = HTG_camDS.indexes['time'].to_datetimeindex()  \n",
    "            \n",
    "    # Discard first two days \n",
    "    iTimeStart   = np.where(HTG_camDS.time.values >= (HTG_camDS.time.values[0] + np.timedelta64(2,'D')))[0]\n",
    "    timeArr      = np.arange(iTimeStart[0], len(HTG_camDS.time.values))\n",
    "\n",
    "    HTG_camDS    = HTG_camDS.isel(time=timeArr)\n",
    "\n",
    "    if fileCount==0:\n",
    "        HTGcamDS_realSfcExtra = HTG_camDS\n",
    "    else: \n",
    "        HTGcamDS_realSfcExtra = xr.concat([HTGcamDS_realSfcExtra,HTG_camDS], dim='time')\n",
    "            \n",
    "    fileCount = fileCount+1\n",
    "    print('Done with file %i of %i '% (fileCount,len(yearStrings)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into larger dataset\n",
    "HTGcamDS_all = xr.merge([HTGcamDS_all, HTGcamDS_realSfcExtra])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do the processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done computing PBL depth with theta\n"
     ]
    }
   ],
   "source": [
    "## Add evaporative fraction to DS \n",
    "HTG_EF = HTGcamDS_all.LHFLX.values / (HTGcamDS_all.LHFLX.values + HTGcamDS_all.SHFLX.values)\n",
    "HTGcamDS_all['EvapFraction'] = (('time'), np.squeeze(HTG_EF))\n",
    "\n",
    "## Define the actual vertical velocity skew, not just the third order moment \n",
    "skw_W_HTG = HTGcamDS_all.WP3_CLUBB.values / ((HTGcamDS_all.WP2_CLUBB.values)**1.5)\n",
    "HTGcamDS_all['Skw_W'] = (('time','ilev'), np.squeeze(skw_W_HTG))\n",
    "\n",
    "\n",
    "## Steps to get PBL \n",
    "\n",
    "## Get *potential* temperature, not just T \n",
    "\n",
    "# So first, get actual pressures \n",
    "p0       = HTGcamDS_all['P0'].values[0]\n",
    "plevmHTG = HTGcamDS_all['hyam']*p0 + HTGcamDS_all['hybm']*HTGcamDS_all['PS'].isel(lat=0,lon=0) # Mid level\n",
    "plevmHTG.attrs['units'] = \"Pa\"\n",
    "\n",
    "HTG_theta = np.asarray(mpc.potential_temperature(plevmHTG * units.pascals, HTGcamDS_all['T'] * units.kelvin))\n",
    "\n",
    "# Add to existing DS\n",
    "HTGcamDS_all['theta'] = (('time','lev','lat','lon'), HTG_theta)\n",
    "\n",
    "# Height with standard atmosphere\n",
    "zlevmHTG_vals = 1000.*np.asarray(mpc.pressure_to_height_std(plevmHTG)) # Units of [m] after multiplied \n",
    "zlevmHTG      = plevmHTG.copy(deep=True)\n",
    "zlevmHTG[:,:] = zlevmHTG_vals\n",
    "\n",
    "# Now compute the BL depth and save it to the larger CAM datasets \n",
    "PBLdepth_htg = PBLHasMaxDZ(HTGcamDS_all, 'theta')\n",
    "print('Done computing PBL depth with theta')\n",
    "\n",
    "# Add above to each dataset\n",
    "HTGcamDS_all['PBLdepth']    = (('time'), PBLdepth_htg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to local time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First four times in UTC:\n",
      " ['2015-06-02T23:32:15.000000000' '2015-06-02T23:37:15.000000000'\n",
      " '2015-06-02T23:42:15.000000000' '2015-06-02T23:47:15.000000000'\n",
      " '2015-06-02T23:52:15.000000000']\n",
      "First four times in local:\n",
      " ['2015-06-02T18:32:15.000000000' '2015-06-02T18:37:15.000000000'\n",
      " '2015-06-02T18:42:15.000000000' '2015-06-02T18:47:15.000000000'\n",
      " '2015-06-02T18:52:15.000000000']\n"
     ]
    }
   ],
   "source": [
    "## Convert to local times...\n",
    "HTGcamDS_localReal      = HTGcamDS_all.copy(deep=True)\n",
    "HTGclmDS_localReal      = HTGclmDS_realSfc.copy(deep=True)\n",
    "HTGclmDS_localRealPatch = HTGclmDS_realSfcPatch.copy(deep=True)\n",
    "\n",
    "\n",
    "# Confirmed that all the times are identical, so using the same local time arrays\n",
    "localTimes = HTGcamDS_all['time'].values - np.timedelta64(5,'h')\n",
    "\n",
    "# Replace time dimension with local time\n",
    "HTGcamDS_localReal      = HTGcamDS_localReal.assign_coords({\"time\": localTimes})\n",
    "HTGclmDS_localReal      = HTGclmDS_localReal.assign_coords({\"time\": localTimes})\n",
    "HTGclmDS_localRealPatch = HTGclmDS_localRealPatch.assign_coords({\"time\": localTimes})\n",
    "\n",
    "\n",
    "print('First four times in UTC:\\n', HTGcamDS_all.time.values[0:5])\n",
    "print('First four times in local:\\n', HTGcamDS_localReal.time.values[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save CLM files out to a pickle:\n",
    "saveDir = '/Users/mdfowler/Documents/Analysis/CLASP/SCAM_runs/FullyCoupledFromCTSM/FinalOption/ProcessedFiles/'\n",
    "\n",
    "pickle.dump( HTGclmDS_localReal,      open( saveDir+\"HTGclmDS_localReal0p0.p\", \"wb\" ) )\n",
    "pickle.dump( HTGclmDS_localRealPatch, open( saveDir+\"HTGclmDS_localRealPatch0p0.p\", \"wb\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Belated realization that the heights computed are above *sea level* not above ground level. \n",
    "#    Need to subtract elevation. \n",
    "nateFile = '/Users/mdfowler/Documents/Analysis/CLASP/NateForcing/clasp-hmg.bdate.nc'\n",
    "nateDS = xr.open_dataset(nateFile, decode_times=True)\n",
    "elevation = nateDS.alt.values\n",
    "\n",
    "HTGcamDS_localReal['PBLdepth']   = HTGcamDS_localReal['PBLdepth']  - elevation[0][0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add in TKE \n",
    "HTGcamDS_localReal['TKE']  = (('time','ilev','lat,','lon'),\n",
    "                   0.5*(HTGcamDS_localReal['UP2_CLUBB']+HTGcamDS_localReal['VP2_CLUBB']+HTGcamDS_localReal['WP2_CLUBB'])) \n",
    "\n",
    "HTGcamDS_localReal['TKE'].attrs['units']   = 'm2/s2'\n",
    "HTGcamDS_localReal['TKE'].attrs['long_name']   = 'Turbulent Kinetic Energy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add in wind speed \n",
    "HTGcamDS_localReal['WindMagnitude']  = (('time','lev','lat,','lon'),\n",
    "                                np.sqrt((HTGcamDS_localReal.U.values**2.0) + (HTGcamDS_localReal.V.values**2.0)) )\n",
    "\n",
    "HTGcamDS_localReal['WindMagnitude'].attrs['units']   = 'm/s'\n",
    "HTGcamDS_localReal['WindMagnitude'].attrs['long_name']   = 'Wind speed'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpolate to standard pressure levels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with variable 0 of 25\n",
      "Done with variable 1 of 25\n",
      "Done with variable 2 of 25\n",
      "Done with variable 3 of 25\n",
      "Done with variable 4 of 25\n",
      "Done with variable 5 of 25\n",
      "Done with variable 6 of 25\n",
      "Done with variable 7 of 25\n",
      "Done with variable 8 of 25\n",
      "Done with variable 9 of 25\n",
      "Done with variable 10 of 25\n",
      "Done with variable 11 of 25\n",
      "Done with variable 12 of 25\n",
      "Done with variable 13 of 25\n",
      "Done with variable 14 of 25\n",
      "Done with variable 15 of 25\n",
      "Done with variable 16 of 25\n",
      "Done with variable 17 of 25\n",
      "Done with variable 18 of 25\n",
      "Done with variable 19 of 25\n",
      "Done with variable 20 of 25\n",
      "Done with variable 21 of 25\n",
      "Done with variable 22 of 25\n",
      "Done with variable 23 of 25\n",
      "Done with variable 24 of 25\n"
     ]
    }
   ],
   "source": [
    "## Decide on levels to interpoalte to and add to larger arrays\n",
    "pnew64 = np.arange(200.0,980.0,10.0) \n",
    "\n",
    "HTGcamDS_localReal = HTGcamDS_localReal.assign_coords({\"levInterp\": pnew64})\n",
    "\n",
    "varSels = np.asarray(['THLP2_CLUBB','RTP2_CLUBB','RTPTHLP_CLUBB','WPRTP_CLUBB','WPTHLP_CLUBB','WP2_CLUBB','UP2_CLUBB',\n",
    "                      'VP2_CLUBB','TKE','U','V','T','Q','OMEGA','RVMTEND_CLUBB','STEND_CLUBB','CLOUD',\n",
    "                      'UPWP_CLUBB','VPWP_CLUBB','WP2RTP_CLUBB','THETAL','QRL','QRS','DCQ','WindMagnitude'])\n",
    "\n",
    "\n",
    "for iVar in range(len(varSels)): \n",
    "    varUnits = HTGcamDS_localReal[varSels[iVar]].units\n",
    "    varName  = HTGcamDS_localReal[varSels[iVar]].long_name\n",
    "    \n",
    "    # Interpolate variables and add to larger arrays \n",
    "    interpVar_real = interpolateToPressure_v2(HTGcamDS_localReal,     varSels[iVar], pnew64)\n",
    "    \n",
    "    HTGcamDS_localReal[varSels[iVar]+'_interp']  = (('time','levInterp','lat','lon'), interpVar_real)\n",
    "     \n",
    "    ## Assign attibutes \n",
    "    HTGcamDS_localReal[varSels[iVar]+'_interp'].attrs['units']     = varUnits\n",
    "    HTGcamDS_localReal[varSels[iVar]+'_interp'].attrs['long_name'] = varName\n",
    "\n",
    "    \n",
    "    print('Done with variable %i of %i' % (iVar, len(varSels)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved interp CAM files to pickle\n"
     ]
    }
   ],
   "source": [
    "saveDir = '/Users/mdfowler/Documents/Analysis/CLASP/SCAM_runs/FullyCoupledFromCTSM/FinalOption/ProcessedFiles/'\n",
    "pickle.dump( HTGcamDS_localReal,   open( saveDir+\"realSfc_HTG0p0_withInterp.p\", \"wb\" ) )\n",
    "print('Saved interp CAM files to pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveDir = '/Users/mdfowler/Documents/Analysis/CLASP/SCAM_runs/FullyCoupledFromCTSM/FinalOption/ProcessedFiles/'\n",
    "\n",
    "# realSfc_allCases   = pickle.load( open( saveDir+\"realSfc_allCases_withInterp.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTGcamDS_localReal = HTGcamDS_localReal.assign_coords({\"case\": 'HTGp5'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list1 = set(realSfc_allCases.keys())\n",
    "# list2 = set(HTGcamDS_localReal.keys())\n",
    "# sameVarsCAM = list(list1 & list2)\n",
    "\n",
    "# realSfc_test = xr.concat([realSfc_allCases[sameVarsCAM],\n",
    "#                           HTGcamDS_localReal[sameVarsCAM]], \"case\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
